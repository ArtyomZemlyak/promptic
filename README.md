# promptic
Easy prompt management for python projects

## Concept

Я все думаю над концептом библиотеки. На сколько она вообще нужна и что должна делать.

Изначальные потребности следующие:
- Контекст моделей не безграничный, нужно уметь помещать в него только нужное
- Агенты часто закрытые системы, на которые не повлиять (если это только не собственная разработка), нужно какое-то универсальное решение
- Нужно версионирование промптов

Альтернативы библиотеки:
- Использование несколько вызовов в агента под каждый контекст. Минус: меньше гибкости самого агента
- Использование FC или MCP, агент получает короткую инструкцию, инструменты и может делать что хочет далее. Минус: излишнее кол-во тулзов, необходимость их разворота и поддержки

Концептуальные мысли:
- Все делаем через файлы (контекст, промпт, память, задачи и тд)
- Агент читает только то что нужно в моменте, остальное удаляет из контекста (сам или алгоритмически)

Проблемы:
- Многие агенты сами как-то менеджерят контекст свой. И даже если агет читает файлы последовательно под задачи, то его контекст все равно будет наполняться содержимым этих файлов. А очищение может наступить только при переполнении. То есть, либа поможет, но только частично.
- Многие агенты это закрытые ящики и python либа для них будет бесполезна. Но, можно пойти следующим путем: задача для агента ставится какой-то системой, которая как раз может использовать python (тут все ок); если же python никак не применим, то формат после render_for_llm промтпов итоговый должен быть легко понятен агентам (И тут можно в теории самим промпт заполнять или с помощью нашей либы).

Нюансы относительно текущего использования:
- То что у нас много вариантов формирования итогового контекста это интересно, пока оставим (но не факт что в итоге нам это нужно будет).
- Нужно нацелиться именно на уменьшение контекста подаваемого в ЛЛМ в агента исходным промптом, если смотреть на render_for_llm, то там так сейчас не работает, там просто рендерится весь промпт полностью.

Нужно чтобы библиотека умела делать что-то такое:
Все файлы такие как есть сейчас - это хорошо.
Далее в LLM передается только ключевой промпт, ключевая инструкция, список шагов и уточнения какие-нибудь.
И плюсом где нужно указывается, что вот там смотри подробнее. То есть мы идем file-first промптинг или как-то так.
В итоге это может выглядеть вот так (образный пример основного промпта и инструкций):

```md
Ты ассистент. Инструкции:
- Ты умный
- Ты крутой
- Не пиши глупости

Тебе нужно сделать:
1. Подумать (подробнее - instructions/think.md)
2. Написать промежуточные выводы (подробнее - instructions/semi.md)
3. Написать итоговый вывод (подробнее - instructions/total.md)
4. Добавить медиа (подробнее - instructions/media.md)

Если нужно что-то запоминать пиши сюда: memory (формат: такой-то там)
```
И допустим какой-нибудь instructions/media.md файл:

```md
Список медиа:
1
2
3
4
...

Если хочешь узнать подробности, OCR или транскрипцию конкретного медиа, то заменяй файловое расширение в названии файла медиа на .md и читай этот файл.
```

То есть, вот она иерархичность, которую я подразумевал изначально - иерархичность промпта, контекста, памяти через файлы.
Поэтому нужно делать 003 фичу с переформатированием / дополнением нашей библиотеки.
